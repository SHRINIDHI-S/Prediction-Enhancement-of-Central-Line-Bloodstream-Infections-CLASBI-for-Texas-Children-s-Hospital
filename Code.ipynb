{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xxk7lElpdJwK"
   },
   "source": [
    "# **Capstone 2 : Final Script**\n",
    "\n",
    "\n",
    "\n",
    "* Shrinidhi Sudhir   \n",
    "* Duyen Do\n",
    "* Lakshmi Kambathanahally Lakshminarasapa\n",
    "* Jessica Nguyen\n",
    "\n",
    "\n",
    "\n",
    "### **Detailed Code Analysis for CLABSI Data Exploration and Preprocessing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC907lNxQo3H"
   },
   "source": [
    "This report breaks down the Python code used for data exploration, preprocessing, and preparation for machine learning modeling. The dataset contains clinical data relevant to CLABSI (Central Line Associated Blood Stream Infections).\n",
    "\n",
    "### Setup\n",
    "\n",
    "First, we import necessary libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KLmWnf28hKZ9",
    "outputId": "be41c387-9d69-4257-ac50-1a5a0886fc79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-4fa8efa7ce64>:21: DtypeWarning: Columns (10,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('/content/bzan6361_clabsi_1.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imblearn\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('/content/bzan6361_clabsi_1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QgjlbcjRTBY"
   },
   "source": [
    "**Data Inspection**\n",
    "\n",
    "Here, we display the first few entries of the dataset and basic information such as column datatypes and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REUwCfNXlWwj",
    "outputId": "64f7756f-d2d0-4e59-fa0e-86bcb793fb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Information about the DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14236 entries, 0 to 14235\n",
      "Columns: 278 entries, PatientKey to ICUDaysLast2\n",
      "dtypes: bool(19), float64(19), int64(208), object(32)\n",
      "memory usage: 28.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "data.head()\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(\"Basic Information about the DataFrame:\")\n",
    "print(data.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuc7__gTSPy8"
   },
   "source": [
    "**Understanding the Target Variable**\n",
    "\n",
    "We explore the target variable HasCLABSI which indicates the presence of an infection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAjbsZd0lWtw",
    "outputId": "5faa635e-62c2-4126-aa53-ec18dd3e4d78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Unique Values in 'HasClabsi' Column: 2\n",
      "\n",
      "Count of Each Unique Value in 'HasClabsi' Column:\n",
      "HasCLABSI\n",
      "False    14184\n",
      "True        52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Understanding the target variable\n",
    "unique_values_count = data['HasCLABSI'].nunique()\n",
    "print(\"\\nNumber of Unique Values in 'HasClabsi' Column:\", unique_values_count)\n",
    "\n",
    "unique_values_counts = data['HasCLABSI'].value_counts()\n",
    "print(\"\\nCount of Each Unique Value in 'HasClabsi' Column:\")\n",
    "print(unique_values_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hShM15SPWcPE"
   },
   "source": [
    "### **Zoom In Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApvgGycFSach"
   },
   "source": [
    "**Data Cleaning**\n",
    "\n",
    "\n",
    "\n",
    "*   **Remove Highly Correlated Columns** - We define a function to remove highly correlated numeric columns to reduce multicollinearity.\n",
    "*   **Extract Columns with Max Number Suffix** - This function extracts columns that have numeric suffixes, selecting the maximum number for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ACKTt9UZTZIU"
   },
   "outputs": [],
   "source": [
    "# Define function to remove highly correlated columns, excluding boolean columns\n",
    "def remove_highly_correlated_columns(df, threshold=0.8):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]\n",
    "    to_drop = [column for column in to_drop if column not in df.select_dtypes(include=['bool']).columns]\n",
    "    return df.drop(to_drop, axis=1), to_drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6HSOitTCTfA0"
   },
   "outputs": [],
   "source": [
    "# Define function to extract columns with the max number suffix\n",
    "def extract_max_number_suffix(df, pattern=r'(\\D+)(\\d*)$'):\n",
    "    column_groups = {}\n",
    "    for column in df.columns:\n",
    "        match = re.match(pattern, column)\n",
    "        if match:\n",
    "            prefix, number = match.groups()\n",
    "            if prefix in column_groups:\n",
    "                if number.isdigit():\n",
    "                    column_groups[prefix] = max(column_groups[prefix], int(number))\n",
    "            else:\n",
    "                column_groups[prefix] = int(number) if number.isdigit() else 0\n",
    "    columns_to_keep = [prefix + str(number) if number != 0 else prefix for prefix, number in column_groups.items()]\n",
    "    return df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVy6DnxCTFiX"
   },
   "source": [
    "**Data Transformation**\n",
    "\n",
    "\n",
    "\n",
    "*  Dimensionality Reduction - We apply the previously defined functions to reduce the dimensionality of the DataFrame.\n",
    "*  Output Changes in Dimensionality - Finally, we print the changes in dimensionality and the columns that were dropped.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X2svQ470Rd3W",
    "outputId": "7022c297-7d25-4422-e692-467938fe111d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns before reduction: 278\n",
      "Columns after reduction: 168\n",
      "Dropped columns: ['AlteplaseAdministeredLast3', 'AlteplaseAdministeredLast30', 'AlteplaseAdministeredLast5', 'CHGBathsLast3', 'CHGBathsLast30', 'CHGBathsLast5', 'CapChangesLast7', 'CountMedicationsLast2', 'CountMedicationsLast3', 'CountMedicationsLast30', 'CountMedicationsLast5', 'DiagnosisClotLast3', 'DressingChangesLast7', 'FlushedLast4', 'FlushedLast7', 'HCLAdministeredLast2', 'HCLAdministeredLast3', 'HCLAdministeredLast30', 'HCLAdministeredLast5', 'ICUDaysLast2', 'ICUDaysLast3', 'ICUDaysLast30', 'ICUDaysLast5', 'LineDaysMultiLumen', 'LineDaysPort', 'LineDaysUpperArm', 'MedicationsInjectedLast3', 'MedicationsInjectedLast30', 'MedicationsInjectedLast5', 'MedsAcidSuppTherapyLast2', 'MedsAcidSuppTherapyLast3', 'MedsAcidSuppTherapyLast30', 'MedsAcidSuppTherapyLast5', 'MedsBowelRegimenLast3', 'MedsBowelRegimenLast30', 'MedsBowelRegimenLast5', 'MedsCentralTPNLast3', 'MedsCentralTPNLast30', 'MedsCentralTPNLast5', 'MedsFatEmulsionLast3', 'MedsFatEmulsionLast30', 'MedsFatEmulsionLast5', 'MedsH2RALast3', 'MedsH2RALast30', 'MedsH2RALast5', 'MedsNSAIDLast3', 'MedsNSAIDLast30', 'MedsNSAIDLast5', 'MedsOralCareLast2', 'MedsOralCareLast3', 'MedsOralCareLast30', 'MedsOralCareLast5', 'MedsPPILast2', 'MedsPPILast3', 'MedsPPILast30', 'MedsPPILast5', 'MedsPropofolLast3', 'MedsPropofolLast5', 'MedsSteroidsLast3', 'MedsSteroidsLast30', 'MedsSteroidsLast5', 'SedatedLast3', 'SedatedLast30', 'SedatedLast5', 'SurgeriesAbdominalLast3', 'SurgeriesBloodLast3', 'SurgeriesBloodLast30', 'SurgeriesCancerLast15', 'SurgeriesCancerLast2', 'SurgeriesCancerLast30', 'SurgeriesCancerLast5', 'SurgeriesIntestinalLast30', 'SurgeriesPlasticLast3', 'SurgeriesPlasticLast30', 'SurgeriesSingleVentricleLast3', 'SurgeriesSingleVentricleLast5', 'SurgeryCountLast30', 'TubingChangesLast7']\n"
     ]
    }
   ],
   "source": [
    "# Apply functions to reduce the dimensionality of the DataFrame\n",
    "non_boolean_numerical_columns = data.select_dtypes(include=[np.number]).columns.difference(data.select_dtypes(include=['bool']).columns)\n",
    "data_numeric, dropped_columns = remove_highly_correlated_columns(data[non_boolean_numerical_columns])\n",
    "\n",
    "# Directly concatenate the processed numeric columns with the original boolean columns from `data`\n",
    "boolean_columns = data.select_dtypes(include=['bool']).columns.tolist()\n",
    "data_final = pd.concat([data_numeric, data[boolean_columns]], axis=1)\n",
    "\n",
    "# Output the changes in dimensionality\n",
    "num_columns_before = len(data.columns)\n",
    "num_columns_after = len(data_final.columns)\n",
    "print(\"\\nColumns before reduction:\", num_columns_before)\n",
    "print(\"Columns after reduction:\", num_columns_after)\n",
    "print(\"Dropped columns:\", dropped_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egjGrTjSVJBT"
   },
   "source": [
    "**Categorization of Data by Type**\n",
    "\n",
    "\n",
    "Before addressing missing values, it is important to categorize columns by their data type for appropriate handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ItDNj_tmRd5z"
   },
   "outputs": [],
   "source": [
    "int_columns = data_final.select_dtypes(include='int64')\n",
    "object_columns = data_final.select_dtypes(include='object')\n",
    "bool_columns = data_final.select_dtypes(include='bool')\n",
    "float_columns = data_final.select_dtypes(include='float64')\n",
    "datetime_columns = data_final.select_dtypes(include='datetime64[ns]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6dCQfNfVXC4"
   },
   "source": [
    "**Identifying Missing Values**\n",
    "\n",
    "We systematically identify missing values across different data types to ensure completeness of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VswNPn0XRd8G",
    "outputId": "6057b961-3e88-48e9-e993-5899d368e82f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer Columns with Missing Values:\n",
      "\n",
      "Object Columns with Missing Values:\n",
      "\n",
      "Boolean Columns with Missing Values:\n",
      "\n",
      "Float Columns with Missing Values:\n",
      "CapChangesLast10: 1472 missing values\n",
      "CapChangesLast4: 1472 missing values\n",
      "CapChangesLastToday: 1472 missing values\n",
      "DaysToCLABSI: 13143 missing values\n",
      "DressingChangesLast10: 1472 missing values\n",
      "DressingChangesLast4: 1472 missing values\n",
      "DressingChangesLastToday: 1472 missing values\n",
      "FlushedLast10: 1472 missing values\n",
      "FlushedToday: 1472 missing values\n",
      "LastAnc: 198 missing values\n",
      "LastAncDelta: 1134 missing values\n",
      "TubingChangesLast10: 1472 missing values\n",
      "TubingChangesLast4: 1472 missing values\n",
      "TubingChangesLastToday: 1472 missing values\n",
      "\n",
      "Datetime Columns with Missing Values:\n"
     ]
    }
   ],
   "source": [
    "# Displaying columns with missing values and their count for each data type\n",
    "print(\"Integer Columns with Missing Values:\")\n",
    "for column in int_columns.columns[int_columns.isnull().any()]:\n",
    "    print(f\"{column}: {int_columns[column].isnull().sum()} missing values\")\n",
    "\n",
    "print(\"\\nObject Columns with Missing Values:\")\n",
    "for column in object_columns.columns[object_columns.isnull().any()]:\n",
    "    print(f\"{column}: {object_columns[column].isnull().sum()} missing values\")\n",
    "\n",
    "print(\"\\nBoolean Columns with Missing Values:\")\n",
    "for column in bool_columns.columns[bool_columns.isnull().any()]:\n",
    "    print(f\"{column}: {bool_columns[column].isnull().sum()} missing values\")\n",
    "\n",
    "print(\"\\nFloat Columns with Missing Values:\")\n",
    "for column in float_columns.columns[float_columns.isnull().any()]:\n",
    "    print(f\"{column}: {float_columns[column].isnull().sum()} missing values\")\n",
    "\n",
    "print(\"\\nDatetime Columns with Missing Values:\")\n",
    "for column in datetime_columns.columns[datetime_columns.isnull().any()]:\n",
    "    print(f\"{column}: {datetime_columns[column].isnull().sum()} missing values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXbqMO5NVfU1"
   },
   "source": [
    "**Handling Missing Values**\n",
    "\n",
    "Considering the skewness of the data, missing values in numerical columns are filled with the median of the respective group based on the HasCLABSI status. This approach mitigates the impact of outliers affecting the imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jDvHQproRd_F",
    "outputId": "a0653e3d-59e4-41f3-99de-b30d7b154e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after filling:\n",
      "AlteplaseAdministeredLast15    0\n",
      "SurgeriesAbdominalLast5        0\n",
      "PICUDaysLast30                 0\n",
      "PastCLABSIs                    0\n",
      "PatientKey                     0\n",
      "                              ..\n",
      "FlushedToday                   0\n",
      "HCLAdministeredLast15          0\n",
      "HospitalDay                    0\n",
      "ICUDaysLast15                  0\n",
      "DiagnosisSwellingLast2         0\n",
      "Length: 168, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in object columns with 'NA'\n",
    "data_final[object_columns.columns] = data_final[object_columns.columns].fillna('NA')\n",
    "\n",
    "# Filling missing values in float and integer columns with medians based on 'HasCLABSI' status\n",
    "for column in float_columns.columns:\n",
    "    data_final.loc[data_final['HasCLABSI'] == True, column] = data_final.loc[data_final['HasCLABSI'] == True, column].fillna(data_final.loc[data_final['HasCLABSI'] == True, column].median())\n",
    "    data_final.loc[data_final['HasCLABSI'] == False, column] = data_final.loc[data_final['HasCLABSI'] == False, column].fillna(data_final.loc[data_final['HasCLABSI'] == False, column].median())\n",
    "\n",
    "for column in int_columns.columns:\n",
    "    data_final.loc[data_final['HasCLABSI'] == True, column] = data_final.loc[data_final['HasCLABSI'] == True, column].fillna(data_final.loc[data_final['HasCLABSI'] == True, column].median())\n",
    "    data_final.loc[data_final['HasCLABSI'] == False, column] = data_final.loc[data_final['HasCLABSI'] == False, column].fillna(data_final.loc[data_final['HasCLABSI'] == False, column].median())\n",
    "\n",
    "# Re-checking for any remaining missing values\n",
    "missing_values_after_filling = data_final.isnull().sum().sort_values(ascending=False)\n",
    "if missing_values_after_filling.empty:\n",
    "    print(\"No missing values remain.\")\n",
    "else:\n",
    "    print(\"Missing values after filling:\")\n",
    "    print(missing_values_after_filling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fj7wcuGrVoJc"
   },
   "source": [
    "**Date Handling and Sorting**\n",
    "\n",
    "\n",
    "The Date column is converted to datetime format, and the data is sorted by PatientKey and Date to prepare for shifting operations in creating lagged variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DCnIuNHiReBS"
   },
   "outputs": [],
   "source": [
    "# Ensure 'Date' column is in datetime format and sort data\n",
    "data_final['Date'] = pd.to_datetime(data['Date'])\n",
    "data_final.sort_values(by=['PatientKey', 'Date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osywwrCGV2oj"
   },
   "source": [
    "**Creating Predictive Variables**\n",
    "\n",
    "\n",
    "Creating variables to predict CLABSI occurrences within one and three days using data shifting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "69H8q0rJReDg"
   },
   "outputs": [],
   "source": [
    "# Ensure 'Date' column is in datetime format\n",
    "data_final['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "# Sort by 'PatientKey' and 'Date'\n",
    "data_final.sort_values(by=['PatientKey', 'Date'], inplace=True)\n",
    "\n",
    "# Assuming CLABSI is recorded the day it occurs and the day prior,\n",
    "# we shift the 'HasCLABSI' column by 1 to get the status of CLABSI for the next day\n",
    "data_final['HasCLABSI_NextDay'] = data_final.groupby('PatientKey')['HasCLABSI'].shift(-1)\n",
    "\n",
    "# Create a new target variable for prediction of CLABSI in the next three days\n",
    "# For simplicity, we'll say a patient is at risk if they will have CLABSI either tomorrow or the two following days\n",
    "data_final['HasCLABSI_Next3Days'] = data_final.groupby('PatientKey')['HasCLABSI'].shift(-1) | data_final.groupby('PatientKey')['HasCLABSI'].shift(-2) | data_final.groupby('PatientKey')['HasCLABSI'].shift(-3)\n",
    "\n",
    "# Handle the NaN values which we'll assume means no CLABSI within the next 3 days\n",
    "data_final['HasCLABSI_Next3Days'].fillna(False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcFLbfMOWp2K"
   },
   "source": [
    "### **Modeling Design**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1Kmj-SsXJAv"
   },
   "source": [
    "**Preparing Data for Predictive Modeling and Evaluating Models**\n",
    "\n",
    "This section describes the process of preparing the dataset for machine learning and evaluating various models to predict the occurrence of CLABSI within three days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXof71InXSsa"
   },
   "source": [
    "**Listing Boolean Columns**\n",
    "\n",
    "\n",
    "First, identify all boolean columns in the data_final DataFrame. These columns represent categorical data in binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "csFFJMauReGA",
    "outputId": "f09a1987-96f0-42a8-a09c-68acbbf57891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HasCLABSI', 'HasFutureEncounterCLABSI', 'HadPreviousCLABSI', 'HasRecentLDAFlowsheetRecords', 'DiagnosisLeukemiaLast30', 'DiagnosisLeukemiaLast15', 'DiagnosisLeukemiaLast5', 'DiagnosisLeukemiaLast3', 'DiagnosisLeukemiaLast2', 'DiagnosisTransplantLast30', 'DiagnosisTransplantLast15', 'DiagnosisTransplantLast5', 'DiagnosisTransplantLast3', 'DiagnosisTransplantLast2', 'DiagnosisSwellingLast30', 'DiagnosisSwellingLast15', 'DiagnosisSwellingLast5', 'DiagnosisSwellingLast3', 'DiagnosisSwellingLast2', 'HasCLABSI_Next3Days']\n"
     ]
    }
   ],
   "source": [
    "# List the boolean columns in data_final\n",
    "boolean_columns_in_data_final = data_final.select_dtypes(include=['bool']).columns.tolist()\n",
    "print(boolean_columns_in_data_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2aebM2uXcPm"
   },
   "source": [
    "**Preparing Features and Labels**\n",
    "\n",
    "\n",
    "Prepare the feature matrix X and the target vector y by excluding specific columns that either do not contribute to prediction or leak future information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK8EPgYpReI3",
    "outputId": "f97d2c25-6e7b-4c26-94ce-e9fc46a7204b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (14236, 166)\n",
      "Target shape: (14236,)\n"
     ]
    }
   ],
   "source": [
    "# Define columns to exclude from features including datetime or identifiers related to the outcome\n",
    "columns_to_exclude = ['HasCLABSI', 'HasCLABSI_NextDay', 'HasCLABSI_Next3Days', 'Date', 'DaysToCLABSI']\n",
    "# Filter out the columns from 'columns_to_exclude' if they exist in 'data_final'\n",
    "filtered_columns = [col for col in columns_to_exclude if col in data_final.columns]\n",
    "\n",
    "# Prepare the feature matrix X and the target variable y\n",
    "X = data_final.drop(filtered_columns, axis=1, errors='ignore')\n",
    "y = data_final['HasCLABSI_Next3Days'].astype(int)  # Convert the target variable to integer for modeling\n",
    "\n",
    "# Display the final shape of X and y\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsImTH46Xpsa"
   },
   "source": [
    "**Splitting the Dataset**\n",
    "\n",
    "\n",
    "Split the data into training and testing sets to ensure the model is evaluated on unseen data, enhancing the generalizability of the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ld0abozXReK8"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQixH9IZYPzQ"
   },
   "source": [
    "**Defining and Evaluating Models**\n",
    "\n",
    "Define multiple machine learning models to assess which performs best in predicting the risk of CLABSI within the next three days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XU7ibBy7ReNB",
    "outputId": "119e665f-2919-442a-d05b-283be64797f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Neural Network:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2827\n",
      "           1       0.00      0.00      0.00        21\n",
      "\n",
      "    accuracy                           0.99      2848\n",
      "   macro avg       0.50      0.50      0.50      2848\n",
      "weighted avg       0.99      0.99      0.99      2848\n",
      "\n",
      "AUC: 0.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for K-Nearest Neighbors:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2827\n",
      "           1       0.20      0.14      0.17        21\n",
      "\n",
      "    accuracy                           0.99      2848\n",
      "   macro avg       0.60      0.57      0.58      2848\n",
      "weighted avg       0.99      0.99      0.99      2848\n",
      "\n",
      "AUC: 0.871831151986794\n",
      "\n",
      "Results for Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      2827\n",
      "           1       0.04      0.19      0.06        21\n",
      "\n",
      "    accuracy                           0.96      2848\n",
      "   macro avg       0.52      0.58      0.52      2848\n",
      "weighted avg       0.99      0.96      0.97      2848\n",
      "\n",
      "AUC: 0.7649872824970101\n",
      "\n",
      "Results for Decision Tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2827\n",
      "           1       0.59      0.62      0.60        21\n",
      "\n",
      "    accuracy                           0.99      2848\n",
      "   macro avg       0.79      0.81      0.80      2848\n",
      "weighted avg       0.99      0.99      0.99      2848\n",
      "\n",
      "AUC: 0.8079320161032224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary of models\n",
    "additional_models = {\n",
    "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Iterate over each model, fit, and evaluate\n",
    "for name, model in additional_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Display results for each model\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"AUC: {roc_auc_score(y_test, y_proba)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYC0YMepZAZs"
   },
   "source": [
    "### **Evaluation Plan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyC7m4qeZSBk"
   },
   "source": [
    "**Evaluation Plan**\n",
    "\n",
    "This section outlines the evaluation strategies for assessing the performance of predictive models developed to forecast CLABSI occurrences. The plan addresses handling overfitting, dealing with imbalanced data, and selecting appropriate metrics considering the clinical implications of predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yblmLgbvZXj8"
   },
   "source": [
    "**Handling Overfitting**\n",
    "\n",
    "To prevent overfitting and ensure the model generalizes well to new data, we utilize techniques like cross-validation and hyperparameter tuning. We'll use the Decision Tree model as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-uKc44sfRePd",
    "outputId": "676a808a-3e21-4c72-f53b-2f3a6afc17f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'max_depth': 30, 'min_samples_split': 2}\n",
      "Best cross-validation score: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree with GridSearchCV for hyperparameter tuning\n",
    "parameters = {'max_depth': [10, 20, 30], 'min_samples_split': [2, 5, 10]}\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "clf = GridSearchCV(dt, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Best model parameters\n",
    "print(\"Best parameters found: \", clf.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZklMCfaajAl"
   },
   "source": [
    "**Assessing Model Performance in the Face of Imbalanced Data**\n",
    "\n",
    "\n",
    "Given the dataset's likely imbalanced nature, special attention is needed to evaluate the model's ability to predict less frequent events correctly. Here, we use the Naive Bayes model as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rK_ZgMqoReUN",
    "outputId": "f7915ea8-fc40-48b4-ecba-998820e09e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non-CLABSI       0.99      0.96      0.98      2827\n",
      "      CLABSI       0.04      0.19      0.06        21\n",
      "\n",
      "    accuracy                           0.96      2848\n",
      "   macro avg       0.52      0.58      0.52      2848\n",
      "weighted avg       0.99      0.96      0.97      2848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes model for quick evaluation\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "report = classification_report(y_test, y_pred, target_names=['Non-CLABSI', 'CLABSI'])\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX08aeWPawYj"
   },
   "source": [
    "**Context-Specific Evaluation Metrics**\n",
    "\n",
    "\n",
    "Medical predictive modeling must carefully weigh the cost of false positives against false negatives. In this context, metrics that provide insight into these aspects are critical. We utilize the K-Nearest Neighbors model to illustrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2x5LduUlWpR",
    "outputId": "be244ba0-eb72-43c2-dae2-b721785729e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# K-Nearest Neighbors model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "y_proba = knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# AUC for model evaluation\n",
    "auc_score = roc_auc_score(y_test, y_proba)\n",
    "print(\"AUC: {:.2f}\".format(auc_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzrrFZmabvNz"
   },
   "source": [
    "**Handling Imbalanced Data with SMOTE**\n",
    "\n",
    "To address the challenge of imbalanced data, which is common in medical datasets where one class (e.g., patients with CLABSI) is significantly underrepresented, we will use SMOTE. This technique generates synthetic samples from the minority class to achieve balance between the classes, which can improve model performance, especially for algorithms that are sensitive to class imbalance.\n",
    "\n",
    "\n",
    "\n",
    "**Implementing SMOTE with Decision Tree Classifier**\n",
    "\n",
    "We choose the Decision Tree model to demonstrate the effect of SMOTE, as decision trees can often overfit to the majority class in their default settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vvaZrFIbbv6k",
    "outputId": "7c3651dd-f518-4a05-c309-3a7779217e38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98      2827\n",
      "           1       0.15      0.76      0.25        21\n",
      "\n",
      "    accuracy                           0.97      2848\n",
      "   macro avg       0.58      0.87      0.62      2848\n",
      "weighted avg       0.99      0.97      0.98      2848\n",
      "\n",
      "ROC AUC Score: 0.9117775868748632\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Applying SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Training the Decision Tree model on SMOTE-enhanced data\n",
    "dt = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "dt.fit(X_train_smote, y_train_smote)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, dt.predict_proba(X_test)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7by743wcrIw"
   },
   "source": [
    "\n",
    "1. **High Recall (0.76)**: Your model successfully identifies 76% of actual CLABSI cases, which is crucial in medical applications where missing a true positive can have serious consequences.\n",
    "2. **Low Precision (0.15)**: The model produces many false positives; only 15% of predictions for CLABSI are correct. This means a lot of the predictions indicating CLABSI are false alarms.\n",
    "3. **ROC AUC Score (0.9118)**: This high score suggests that the model is generally good at distinguishing between patients with and without CLABSI, but it might be setting the classification threshold too low, leading to many false positives.\n",
    "\n",
    "**Interpretation:**\n",
    "- **Trade-off Between Precision and Recall**: The model is tuned to avoid missing CLABSI cases (high recall) at the cost of higher false positives (low precision). This might be acceptable in some medical scenarios where failing to detect a condition is worse than false alarms.\n",
    "  \n",
    "- **Model and Threshold Adjustment**: Adjusting the classification threshold to reduce false positives or experimenting with different models or tuning parameters to find a better balance between precision and recall.\n",
    "\n",
    "The model's current setup prioritizes minimizing missed cases of CLABSI, which is typical in medical diagnostics. However, depending on the consequences of false positives in your specific application,the model is finetuned or its decision threshold to balance the outcomes better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
